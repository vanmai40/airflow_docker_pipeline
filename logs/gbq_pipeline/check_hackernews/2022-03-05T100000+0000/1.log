[2022-06-06 15:23:31,427] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 15:23:31,450] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 15:23:31,451] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 15:23:31,451] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 15:23:31,451] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 15:23:31,458] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-03-05T10:00:00+00:00
[2022-06-06 15:23:31,462] {standard_task_runner.py:52} INFO - Started process 255 to run task
[2022-06-06 15:23:31,467] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-03-05T10:00:00+00:00', '--job-id', '384', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmprq782j4k', '--error-file', '/tmp/tmpun48wlvn']
[2022-06-06 15:23:31,469] {standard_task_runner.py:77} INFO - Job 384: Subtask check_hackernews
[2022-06-06 15:23:31,507] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 15:23:31,540] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-03-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-05T10:00:00+00:00
[2022-06-06 15:23:31,542] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220305" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 15:23:31,551] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 15:23:31,928] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 15:23:31,930] {bigquery.py:1510} INFO - Inserting job airflow_1654529011929441_af20bf2c0e425ec4d435ce205c21373e
[2022-06-06 15:23:34,076] {sql.py:90} INFO - Record: [True]
[2022-06-06 15:23:34,077] {sql.py:96} INFO - Success.
[2022-06-06 15:23:34,085] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220305T100000, start_date=20220606T152331, end_date=20220606T152334
[2022-06-06 15:23:34,241] {taskinstance.py:1220} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-06 15:23:34,250] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 16:25:24,622] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:25:24,640] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:25:24,640] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:25:24,640] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 16:25:24,640] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:25:24,647] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-03-05T10:00:00+00:00
[2022-06-06 16:25:24,651] {standard_task_runner.py:52} INFO - Started process 392 to run task
[2022-06-06 16:25:24,654] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-03-05T10:00:00+00:00', '--job-id', '429', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmp_70xq5y3', '--error-file', '/tmp/tmpuje1_s4o']
[2022-06-06 16:25:24,656] {standard_task_runner.py:77} INFO - Job 429: Subtask check_hackernews
[2022-06-06 16:25:24,690] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 16:25:24,722] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-03-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-05T10:00:00+00:00
[2022-06-06 16:25:24,723] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220305" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 16:25:24,730] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 16:25:25,178] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 16:25:25,180] {bigquery.py:1510} INFO - Inserting job airflow_1654532725179512_af20bf2c0e425ec4d435ce205c21373e
[2022-06-06 16:25:26,718] {sql.py:90} INFO - Record: [True]
[2022-06-06 16:25:26,718] {sql.py:96} INFO - Success.
[2022-06-06 16:25:26,726] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220305T100000, start_date=20220606T162524, end_date=20220606T162526
[2022-06-06 16:25:26,752] {taskinstance.py:1220} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-06 16:25:26,795] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 16:56:01,593] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:56:01,609] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:56:01,610] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:56:01,610] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 16:56:01,610] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:56:01,616] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-03-05T10:00:00+00:00
[2022-06-06 16:56:01,619] {standard_task_runner.py:52} INFO - Started process 498 to run task
[2022-06-06 16:56:01,622] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-03-05T10:00:00+00:00', '--job-id', '465', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmpft67k39g', '--error-file', '/tmp/tmpl7lrjxs0']
[2022-06-06 16:56:01,624] {standard_task_runner.py:77} INFO - Job 465: Subtask check_hackernews
[2022-06-06 16:56:01,656] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 16:56:01,685] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-03-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-05T10:00:00+00:00
[2022-06-06 16:56:01,687] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220305" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 16:56:01,696] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 16:56:02,043] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 16:56:02,044] {bigquery.py:1510} INFO - Inserting job airflow_1654534562044141_af20bf2c0e425ec4d435ce205c21373e
[2022-06-06 16:56:03,541] {sql.py:90} INFO - Record: [True]
[2022-06-06 16:56:03,541] {sql.py:96} INFO - Success.
[2022-06-06 16:56:03,548] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220305T100000, start_date=20220606T165601, end_date=20220606T165603
[2022-06-06 16:56:03,758] {taskinstance.py:1220} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-06 16:56:03,767] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 16:57:55,403] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:57:55,420] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:57:55,421] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:57:55,421] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 16:57:55,422] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:57:55,427] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-03-05T10:00:00+00:00
[2022-06-06 16:57:55,434] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-03-05T10:00:00+00:00', '--job-id', '505', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmpb1qdoa4m', '--error-file', '/tmp/tmpjhij7rzh']
[2022-06-06 16:57:55,432] {standard_task_runner.py:52} INFO - Started process 618 to run task
[2022-06-06 16:57:55,436] {standard_task_runner.py:77} INFO - Job 505: Subtask check_hackernews
[2022-06-06 16:57:55,468] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 16:57:55,496] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-03-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-05T10:00:00+00:00
[2022-06-06 16:57:55,497] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220305" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 16:57:55,504] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 16:57:55,978] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 16:57:55,979] {bigquery.py:1510} INFO - Inserting job airflow_1654534675978815_af20bf2c0e425ec4d435ce205c21373e
[2022-06-06 16:57:57,228] {sql.py:90} INFO - Record: [True]
[2022-06-06 16:57:57,229] {sql.py:96} INFO - Success.
[2022-06-06 16:57:57,240] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220305T100000, start_date=20220606T165755, end_date=20220606T165757
[2022-06-06 16:57:57,264] {taskinstance.py:1220} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-06-06 16:57:57,300] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 16:59:54,477] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:59:54,497] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [queued]>
[2022-06-06 16:59:54,497] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:59:54,498] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 16:59:54,498] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 16:59:54,515] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-03-05T10:00:00+00:00
[2022-06-06 16:59:54,522] {standard_task_runner.py:52} INFO - Started process 729 to run task
[2022-06-06 16:59:54,535] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-03-05T10:00:00+00:00', '--job-id', '539', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmpnau8gjh0', '--error-file', '/tmp/tmpz1c250k0']
[2022-06-06 16:59:54,538] {standard_task_runner.py:77} INFO - Job 539: Subtask check_hackernews
[2022-06-06 16:59:54,616] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-03-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 16:59:54,674] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-03-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-05T10:00:00+00:00
[2022-06-06 16:59:54,676] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220305" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 16:59:54,688] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 16:59:55,300] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 16:59:55,302] {bigquery.py:1510} INFO - Inserting job airflow_1654534795301524_af20bf2c0e425ec4d435ce205c21373e
[2022-06-06 16:59:57,105] {sql.py:90} INFO - Record: [True]
[2022-06-06 16:59:57,106] {sql.py:96} INFO - Success.
[2022-06-06 16:59:57,116] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220305T100000, start_date=20220606T165954, end_date=20220606T165957
[2022-06-06 16:59:57,230] {taskinstance.py:1220} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-06 16:59:57,280] {local_task_job.py:146} INFO - Task exited with return code 0
