[2022-06-06 15:15:01,766] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 15:15:01,790] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 15:15:01,790] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 15:15:01,790] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 15:15:01,791] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 15:15:01,799] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-06-05T10:00:00+00:00
[2022-06-06 15:15:01,803] {standard_task_runner.py:52} INFO - Started process 174 to run task
[2022-06-06 15:15:01,806] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-06-05T10:00:00+00:00', '--job-id', '357', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmp6j47x338', '--error-file', '/tmp/tmpriqpace3']
[2022-06-06 15:15:01,808] {standard_task_runner.py:77} INFO - Job 357: Subtask check_hackernews
[2022-06-06 15:15:01,844] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 15:15:01,878] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-06-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-05T10:00:00+00:00
[2022-06-06 15:15:01,880] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220605" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 15:15:01,887] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 15:15:02,378] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 15:15:02,379] {bigquery.py:1510} INFO - Inserting job airflow_1654528502378939_931844613688d3a509f625fa42b95a1e
[2022-06-06 15:15:04,630] {sql.py:90} INFO - Record: [True]
[2022-06-06 15:15:04,631] {sql.py:96} INFO - Success.
[2022-06-06 15:15:04,641] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220605T100000, start_date=20220606T151501, end_date=20220606T151504
[2022-06-06 15:15:04,666] {taskinstance.py:1220} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-06 15:15:04,672] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 17:01:56,918] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 17:01:56,941] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 17:01:56,941] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 17:01:56,942] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 17:01:56,942] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 17:01:56,948] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-06-05T10:00:00+00:00
[2022-06-06 17:01:56,953] {standard_task_runner.py:52} INFO - Started process 879 to run task
[2022-06-06 17:01:56,956] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-06-05T10:00:00+00:00', '--job-id', '592', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmps0gqy5be', '--error-file', '/tmp/tmp2rjhtlus']
[2022-06-06 17:01:56,958] {standard_task_runner.py:77} INFO - Job 592: Subtask check_hackernews
[2022-06-06 17:01:56,994] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 17:01:57,026] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-06-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-05T10:00:00+00:00
[2022-06-06 17:01:57,028] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220605" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 17:01:57,035] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 17:01:57,484] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 17:01:57,485] {bigquery.py:1510} INFO - Inserting job airflow_1654534917485010_931844613688d3a509f625fa42b95a1e
[2022-06-06 17:01:58,782] {sql.py:90} INFO - Record: [True]
[2022-06-06 17:01:58,783] {sql.py:96} INFO - Success.
[2022-06-06 17:01:58,790] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220605T100000, start_date=20220606T170156, end_date=20220606T170158
[2022-06-06 17:01:58,817] {taskinstance.py:1220} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-06-06 17:01:58,860] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 17:10:25,460] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 17:10:25,478] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 17:10:25,478] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 17:10:25,479] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 17:10:25,479] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 17:10:25,485] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-06-05T10:00:00+00:00
[2022-06-06 17:10:25,489] {standard_task_runner.py:52} INFO - Started process 964 to run task
[2022-06-06 17:10:25,492] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-06-05T10:00:00+00:00', '--job-id', '621', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmpdlh5_exg', '--error-file', '/tmp/tmpuhxpzltt']
[2022-06-06 17:10:25,493] {standard_task_runner.py:77} INFO - Job 621: Subtask check_hackernews
[2022-06-06 17:10:25,526] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [running]> on host c40819a450e3
[2022-06-06 17:10:25,557] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-06-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-05T10:00:00+00:00
[2022-06-06 17:10:25,558] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220605" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 17:10:25,566] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 17:10:25,987] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 17:10:25,989] {bigquery.py:1510} INFO - Inserting job airflow_1654535425988698_931844613688d3a509f625fa42b95a1e
[2022-06-06 17:10:27,385] {sql.py:90} INFO - Record: [True]
[2022-06-06 17:10:27,385] {sql.py:96} INFO - Success.
[2022-06-06 17:10:27,394] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220605T100000, start_date=20220606T171025, end_date=20220606T171027
[2022-06-06 17:10:27,496] {taskinstance.py:1220} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-06 17:10:27,512] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 17:47:48,127] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 17:47:48,146] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 17:47:48,146] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 17:47:48,147] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 17:47:48,147] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 17:47:48,155] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-06-05T10:00:00+00:00
[2022-06-06 17:47:48,159] {standard_task_runner.py:52} INFO - Started process 382 to run task
[2022-06-06 17:47:48,161] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-06-05T10:00:00+00:00', '--job-id', '761', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmp7krsnw3m', '--error-file', '/tmp/tmpjgkhev5g']
[2022-06-06 17:47:48,163] {standard_task_runner.py:77} INFO - Job 761: Subtask check_hackernews
[2022-06-06 17:47:48,197] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [running]> on host f3f1e04b5b71
[2022-06-06 17:47:48,229] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-06-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-05T10:00:00+00:00
[2022-06-06 17:47:48,230] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220605" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 17:47:48,239] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 17:47:48,840] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 17:47:48,841] {bigquery.py:1510} INFO - Inserting job airflow_1654537668840911_931844613688d3a509f625fa42b95a1e
[2022-06-06 17:47:50,265] {sql.py:90} INFO - Record: [True]
[2022-06-06 17:47:50,266] {sql.py:96} INFO - Success.
[2022-06-06 17:47:50,273] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220605T100000, start_date=20220606T174748, end_date=20220606T174750
[2022-06-06 17:47:50,299] {taskinstance.py:1220} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2022-06-06 17:47:50,344] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 19:50:38,046] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 19:50:38,059] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 19:50:38,059] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 19:50:38,060] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 19:50:38,060] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 19:50:38,070] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-06-05T10:00:00+00:00
[2022-06-06 19:50:38,074] {standard_task_runner.py:52} INFO - Started process 103 to run task
[2022-06-06 19:50:38,077] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-06-05T10:00:00+00:00', '--job-id', '788', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmp3jz40k9o', '--error-file', '/tmp/tmp5y1bz7jp']
[2022-06-06 19:50:38,078] {standard_task_runner.py:77} INFO - Job 788: Subtask check_hackernews
[2022-06-06 19:50:38,110] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [running]> on host e8b9b26156db
[2022-06-06 19:50:38,138] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-06-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-05T10:00:00+00:00
[2022-06-06 19:50:38,140] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220605" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 19:50:38,147] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 19:50:38,722] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 19:50:38,723] {bigquery.py:1510} INFO - Inserting job airflow_1654545038722797_931844613688d3a509f625fa42b95a1e
[2022-06-06 19:50:40,765] {sql.py:90} INFO - Record: [True]
[2022-06-06 19:50:40,766] {sql.py:96} INFO - Success.
[2022-06-06 19:50:40,777] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220605T100000, start_date=20220606T195038, end_date=20220606T195040
[2022-06-06 19:50:40,832] {taskinstance.py:1220} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-06-06 19:50:40,859] {local_task_job.py:146} INFO - Task exited with return code 0
[2022-06-06 20:02:35,497] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 20:02:35,510] {taskinstance.py:851} INFO - Dependencies all met for <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [queued]>
[2022-06-06 20:02:35,510] {taskinstance.py:1042} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 20:02:35,512] {taskinstance.py:1043} INFO - Starting attempt 1 of 3
[2022-06-06 20:02:35,512] {taskinstance.py:1044} INFO - 
--------------------------------------------------------------------------------
[2022-06-06 20:02:35,521] {taskinstance.py:1063} INFO - Executing <Task(BigQueryCheckOperator): check_hackernews> on 2022-06-05T10:00:00+00:00
[2022-06-06 20:02:35,525] {standard_task_runner.py:52} INFO - Started process 289 to run task
[2022-06-06 20:02:35,528] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'gbq_pipeline', 'check_hackernews', '2022-06-05T10:00:00+00:00', '--job-id', '854', '--pool', 'default_pool', '--raw', '--subdir', 'DAGS_FOLDER/gbq_pipeline.py', '--cfg-path', '/tmp/tmpl1um7jka', '--error-file', '/tmp/tmpexz4rl9e']
[2022-06-06 20:02:35,529] {standard_task_runner.py:77} INFO - Job 854: Subtask check_hackernews
[2022-06-06 20:02:35,639] {logging_mixin.py:104} INFO - Running <TaskInstance: gbq_pipeline.check_hackernews 2022-06-05T10:00:00+00:00 [running]> on host e8b9b26156db
[2022-06-06 20:02:35,664] {taskinstance.py:1257} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_EMAIL=airflow@airflow.com
AIRFLOW_CTX_DAG_OWNER=vanmai-airflow
AIRFLOW_CTX_DAG_ID=gbq_pipeline
AIRFLOW_CTX_TASK_ID=check_hackernews
AIRFLOW_CTX_EXECUTION_DATE=2022-06-05T10:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-06-05T10:00:00+00:00
[2022-06-06 20:02:35,665] {sql.py:87} INFO - Executing SQL check: 
        SELECT "20220605" IN
            (
            SELECT FORMAT_TIMESTAMP("%Y%m%d", timestamp ) AS date
            FROM `bigquery-public-data.hacker_news.full`
            WHERE type = 'story'
            )
        
[2022-06-06 20:02:35,672] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:120 DeprecationWarning: This method will be deprecated. Please use `BigQueryHook.get_client` method
[2022-06-06 20:02:36,036] {logging_mixin.py:104} WARNING - /home/airflow/.local/lib/python3.6/site-packages/airflow/providers/google/cloud/hooks/bigquery.py:2052 DeprecationWarning: This method is deprecated. Please use `BigQueryHook.insert_job` method.
[2022-06-06 20:02:36,037] {bigquery.py:1510} INFO - Inserting job airflow_1654545756037080_931844613688d3a509f625fa42b95a1e
[2022-06-06 20:02:37,464] {sql.py:90} INFO - Record: [True]
[2022-06-06 20:02:37,464] {sql.py:96} INFO - Success.
[2022-06-06 20:02:37,471] {taskinstance.py:1166} INFO - Marking task as SUCCESS. dag_id=gbq_pipeline, task_id=check_hackernews, execution_date=20220605T100000, start_date=20220606T200235, end_date=20220606T200237
[2022-06-06 20:02:37,495] {taskinstance.py:1220} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2022-06-06 20:02:37,509] {local_task_job.py:146} INFO - Task exited with return code 0
